{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0_jDk-9SKaZ"
      },
      "source": [
        "# import Python Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j6uGcRAFjxl1"
      },
      "outputs": [],
      "source": [
        "#!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hG1b7z1_No77"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1xq_YisDR54e",
        "outputId": "655fe4a3-042b-4f64-c2f5-2f5a03b208f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'3.8.1'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9z0MwAtSbC1",
        "outputId": "09167308-6e51-46c4-d810-52347c2a683b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TGYVnTopzwA"
      },
      "source": [
        "#Punctiations removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lDS2mQeVi6tX"
      },
      "outputs": [],
      "source": [
        "tweet= 'I have been feeling awful all days. I think I caught the flu from my brother'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRjRhcvOpgHA",
        "outputId": "8bdc8d73-5a50-4932-ed84-fda5468ac356"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I have been feeling awful all days I think I caught the flu from my brother\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "tweet_without_punc = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "print(tweet_without_punc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jt2aAcRrDWd"
      },
      "source": [
        "# case Folding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4j7abc-kAHH",
        "outputId": "1227ba06-f9d8-43ec-cc57-5177c1c572ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i have been feeling awful all days i think i caught the flu from my brother\n"
          ]
        }
      ],
      "source": [
        "tweet_case_fold=tweet_without_punc.lower()\n",
        "print(tweet_case_fold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3e2MPcWp2q4"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zZjw6bhjekR",
        "outputId": "a050a9e6-939c-473b-e9e4-f2348382bc7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'have', 'been', 'feeling', 'awful', 'all', 'days', 'i', 'think', 'i', 'caught', 'the', 'flu', 'from', 'my', 'brother']\n"
          ]
        }
      ],
      "source": [
        "tokens=word_tokenize(tweet_case_fold)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL9ojCtesV55"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXrCY0wfvbMj"
      },
      "source": [
        "# stop words reoval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3jOfX5fv40n",
        "outputId": "b7f8af5b-14b1-4c2a-8381-9a68012f74bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I4YcMDMvc8W",
        "outputId": "2cd55f6a-e23e-4884-aa5d-1c80f2022678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'its', 're', \"you're\", 'mustn', 'aren', 'weren', 'some', 'couldn', 'own', 'being', 'should', 'myself', \"hadn't\", 'than', 'most', 'don', 'there', 'same', \"won't\", 'yourself', 'will', 'her', 'very', 'how', 'i', 'me', 'what', 'that', 'are', 'a', 'on', 'doesn', 'won', 'further', 'only', 'each', 'he', \"weren't\", 'y', 'itself', 'the', \"you'd\", 'did', 'am', \"mightn't\", 'while', 'their', 'having', \"should've\", 'they', 'his', 'no', 'below', 'both', \"isn't\", 'and', 'off', 'by', 'in', 'over', 'yours', 'through', 'wasn', 'with', 'didn', 't', \"aren't\", 'hasn', 'about', 'herself', 'do', 'had', 'be', \"wouldn't\", \"needn't\", 'such', \"you'll\", 'which', 'before', 'from', 've', \"she's\", 'doing', 'under', \"didn't\", 'theirs', 'our', 'once', 's', 'himself', 'to', 'where', \"don't\", 'because', 'hers', 'll', 'does', 'as', 'we', 'themselves', \"that'll\", 'then', 'nor', 'my', 'whom', 'during', 'too', 'this', 'when', 'ourselves', 'more', 'an', 'were', 'can', 'yourselves', 'ain', 'if', 'again', 'not', 'all', 'up', 'mightn', 'for', 'but', 'into', \"doesn't\", 'shan', \"shan't\", 'those', 'down', 'd', 'isn', 'few', \"it's\", 'needn', 'against', 'who', 'or', 'wouldn', 'she', 'at', 'until', 'out', \"mustn't\", 'm', \"haven't\", 'your', 'after', 'why', 'these', 'ours', 'hadn', 'other', \"hasn't\", 'been', 'was', 'is', 'o', 'so', 'now', 'ma', \"wasn't\", \"you've\", 'him', 'any', 'above', \"shouldn't\", 'them', 'you', 'it', 'haven', 'between', \"couldn't\", 'has', 'just', 'of', 'shouldn', 'have', 'here'}\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J1l_j6Kvmvr",
        "outputId": "37c2dc8e-378a-4218-c041-6742a33ca4af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['feeling', 'awful', 'days', 'think', 'caught', 'flu', 'brother']\n"
          ]
        }
      ],
      "source": [
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXi_Yl5S1yZ5"
      },
      "source": [
        "# Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOHVx87l1zKa",
        "outputId": "a8db9170-cea4-431b-f6be-19384d167b6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['feel', 'aw', 'day', 'think', 'caught', 'flu', 'brother']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "stems = []\n",
        "for t in tokens:\n",
        "    stems.append(porter.stem(t))\n",
        "print(stems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2dSHT2r1wli"
      },
      "source": [
        "# Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iacfMFkP2YTr",
        "outputId": "9fe438e4-f3a3-4ea3-f1b9-7ef8e70ebc78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6APopRvwUm_",
        "outputId": "01c0fb05-d15b-4d02-8ee8-5f5c453e9c9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['feeling', 'awful', 'day', 'think', 'caught', 'flu', 'brother']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer= WordNetLemmatizer()\n",
        "\n",
        "lematized_tokens = []\n",
        "for t in tokens:\n",
        "    lematized_tokens.append(lemmatizer.lemmatize(t))\n",
        "print(lematized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVpsqpmDTtWQ"
      },
      "source": [
        "# Part Of Speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE60RRwPTvgk",
        "outputId": "481a72e4-df67-4d6b-ddc0-6a00ae2b0f02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('feeling', 'VBG'), ('awful', 'JJ'), ('days', 'NNS'), ('think', 'VBP'), ('caught', 'VBN'), ('flu', 'NNS'), ('brother', 'NN')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "print(nltk.pos_tag(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G4QNheATJCt"
      },
      "source": [
        "# Lemmatization using POS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3lpCTlP9kgey"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "punctuation = u\",.?!()-_\\\"\\'\\\\\\n\\r\\t;:+*<>@#§^$%&|/\"\n",
        "stop_words_eng = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# A dictionary mapping part-of-speech tags to WordNet part-of-speech constants.\n",
        "tag_dict = {\"J\": wn.ADJ,\n",
        "            \"N\": wn.NOUN,\n",
        "            \"V\": wn.VERB,\n",
        "            \"R\": wn.ADV}\n",
        "\n",
        "# Maps a part-of-speech tag (e.g., 'NN') to its WordNet equivalent.\n",
        "\n",
        "def extract_wnpostag_from_postag(tag):\n",
        "    #take the first letter of the tag\n",
        "    #the second parameter is an \"optional\" in case of missing key in the dictionary\n",
        "\n",
        "    return tag_dict.get(tag[0].upper(), None)\n",
        "\n",
        "\n",
        "\n",
        "def lemmatize_tupla_word_postag(tupla):\n",
        "    \"\"\"\n",
        "    giving a tupla of the form (wordString, posTagString) like ('guitar', 'NN'), return the lemmatized word\n",
        "    \"\"\"\n",
        "    tag = extract_wnpostag_from_postag(tupla[1])\n",
        "    return lemmatizer.lemmatize(tupla[0], tag) if tag is not None else tupla[0]\n",
        "def bag_of_words(sentence, stop_words=None):\n",
        "  # stop_words is optional\n",
        "  # This checks if the stop_words parameter is provided. If not, it sets stop_words to the default set of English stop words (stop_words_eng).\n",
        "    if stop_words is None:\n",
        "        stop_words = stop_words_eng\n",
        "\n",
        "    original_words = word_tokenize(sentence)\n",
        "\n",
        "    tagged_words = nltk.pos_tag(original_words)\n",
        "\n",
        "    # Memory Cleanup:\n",
        "    original_words = None\n",
        "\n",
        "    lemmatized_words = [ lemmatize_tupla_word_postag(ow) for ow in tagged_words ]\n",
        "\n",
        "    # Memory Cleanup:\n",
        "    tagged_words = None\n",
        "\n",
        "    cleaned_words = [ w for w in lemmatized_words if (w not in punctuation) and (w not in stop_words) ]\n",
        "\n",
        "    # Memory Cleanup:\n",
        "    lemmatized_words = None\n",
        "    return cleaned_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHkegjX0uuqq",
        "outputId": "da2c94a4-271d-47ce-c233-0c05f6f7fc1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I', 'feel', 'awful', 'day', 'I', 'think', 'I', 'catch', 'flu', 'brother']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bag_of_words(tweet)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
